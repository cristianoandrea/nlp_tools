{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "eng_path='./en_eslspok-ud-dev.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_rule_based(text):\n",
    "    # Regex patterns for URLs, emails, hashtags\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    email_pattern = r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b'\n",
    "    hashtag_pattern = r'#\\w+'\n",
    "    \n",
    "    # Multi-word expressions (MWEs) you want to capture as single tokens\n",
    "    mwes = [('new', 'york'), ('artificial', 'intelligence')]\n",
    "\n",
    "    # Combine all patterns into a single regex\n",
    "    combined_pattern = f'({url_pattern})|({email_pattern})|({hashtag_pattern})|(\\w+|[^\\w\\s])'\n",
    "\n",
    "    # Step 1: Tokenize using the combined pattern\n",
    "    tokens = re.findall(combined_pattern, text.lower())\n",
    "    tokens = [token for group in tokens for token in group if token]\n",
    "\n",
    "    # Step 2: Handle MWEs manually by scanning for sequences\n",
    "    def join_mwes(tokens, mwes):\n",
    "        mwe_set = set(mwes)  # Convert list of MWEs to set for faster look-up\n",
    "        i = 0\n",
    "        final_tokens = []\n",
    "        while i < len(tokens):\n",
    "            if i + 1 < len(tokens) and (tokens[i], tokens[i + 1]) in mwe_set:\n",
    "                final_tokens.append(f'{tokens[i]} {tokens[i + 1]}')\n",
    "                i += 2  # Skip the next token as it's part of an MWE\n",
    "            else:\n",
    "                final_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        return final_tokens\n",
    "    \n",
    "    # Step 3: Apply MWE handling\n",
    "    tokens = join_mwes(tokens, mwes)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens, stopwords):\n",
    "    return [token for token in tokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_abbreviations(text):\n",
    "    # Replace abbreviations with temporary markers to prevent splitting\n",
    "    abbreviations = {\n",
    "        'mr.': 'MR_TEMP',\n",
    "        'mrs.': 'MRS_TEMP',\n",
    "        'dr.': 'DR_TEMP',\n",
    "        'ms.': 'MS_TEMP',\n",
    "        'etc.': 'ETC_TEMP',\n",
    "        'i.e.': 'IE_TEMP',\n",
    "        'e.g.': 'EG_TEMP',\n",
    "        'u.s.': 'US_TEMP',\n",
    "        'p.m.': 'PM_TEMP',\n",
    "        'a.m.': 'AM_TEMP'\n",
    "    }\n",
    "    for abbr, placeholder in abbreviations.items():\n",
    "        text = text.replace(abbr, placeholder)\n",
    "    return text\n",
    "\n",
    "def postprocess_abbreviations(text):\n",
    "    # Revert the placeholders back to their original abbreviations\n",
    "    abbreviations = {\n",
    "        'MR_TEMP': 'Mr.',\n",
    "        'MRS_TEMP': 'Mrs.',\n",
    "        'DR_TEMP': 'Dr.',\n",
    "        'MS_TEMP': 'Ms.',\n",
    "        'ETC_TEMP': 'etc.',\n",
    "        'IE_TEMP': 'i.e.',\n",
    "        'EG_TEMP': 'e.g.',\n",
    "        'US_TEMP': 'U.S.',\n",
    "        'PM_TEMP': 'p.m.',\n",
    "        'AM_TEMP': 'a.m.'\n",
    "    }\n",
    "    for placeholder, abbr in abbreviations.items():\n",
    "        text = text.replace(placeholder, abbr)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    # Preprocess text to handle abbreviations\n",
    "    text = preprocess_abbreviations(text)\n",
    "    \n",
    "    # Use regex to split sentences on common sentence-ending punctuation\n",
    "    sentence_end_pattern = r'(?<=[.!?])\\s+(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)'\n",
    "    sentences = re.split(sentence_end_pattern, text)\n",
    "    \n",
    "    # Postprocess text to restore abbreviations\n",
    "    sentences = [postprocess_abbreviations(s).strip() for s in sentences if s]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(file_path):\n",
    "    text = read_file(file_path)\n",
    "    tokens = tokenize_rule_based(text)\n",
    "    sentences = split_sentences(text)\n",
    "    stopwords = {\"the\", \"is\", \"in\", \"it\", \"of\", \"and\"}  # Example stopwords\n",
    "    tokens = remove_stopwords(tokens, stopwords)\n",
    "    \n",
    "    return tokens, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './en_eslspok-ud-dev.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokens, sentences \u001b[39m=\u001b[39m process_text(eng_path)\n\u001b[1;32m      2\u001b[0m tokens, \u001b[39mlen\u001b[39m(tokens)\n",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m, in \u001b[0;36mprocess_text\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_text\u001b[39m(file_path):\n\u001b[0;32m----> 2\u001b[0m     text \u001b[39m=\u001b[39m read_file(file_path)\n\u001b[1;32m      3\u001b[0m     tokens \u001b[39m=\u001b[39m tokenize_rule_based(text)\n\u001b[1;32m      4\u001b[0m     sentences \u001b[39m=\u001b[39m split_sentences(text)\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_file\u001b[39m(file_path):\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      3\u001b[0m         text \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './en_eslspok-ud-dev.txt'"
     ]
    }
   ],
   "source": [
    "tokens, sentences = process_text(eng_path)\n",
    "tokens, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"And so this accident became the man on the motorcycle 's fault .\",\n",
       "  'Yes , please .',\n",
       "  'And then think about what we really wanna eat .',\n",
       "  \"the roads are n't this wide ,\\nfirst of all .\",\n",
       "  \"So yeah , I do n't know about others .\",\n",
       "  'he runs every morning\\naround here I like Tom Cruise .',\n",
       "  'And she was looking for a nice gift for him .',\n",
       "  'One girl is sitting on the chair before the desk .',\n",
       "  'But very interesting .',\n",
       "  'in\\nhigher level of English or skills in the area .',\n",
       "  'And ballet is the decide to\\nmovement .',\n",
       "  'because life .',\n",
       "  'My father , my mother , and two sisters .',\n",
       "  'yes , yes .',\n",
       "  \"so I 'm very worried about maybe he think to play good person or try to be good\\nfor adult .\",\n",
       "  'I hope so .',\n",
       "  \"And I just forgot about , you know , giving you the\\nphone that I could n't come .\",\n",
       "  'last weekend my best friend in Tokyo came to Osaka\\nwith his girlfriend to go to Universal Studios in Japan .',\n",
       "  'So I just sleep well .',\n",
       "  'But in here , like people are just like wanting to play around a little more .',\n",
       "  'Osaka City .',\n",
       "  \"something I ca n't do promise it .\",\n",
       "  \"And I 'm going on Sunday class\\nbecause it 's the only available day for me .\",\n",
       "  'In Japan ?',\n",
       "  \"I 'm sorry .\",\n",
       "  'and after\\nthat , I moved to Okinawa in Ishikawa Prefecture to study structure engineering\\nin my university .',\n",
       "  'Thank you .',\n",
       "  \"I do n't know where to stay .\",\n",
       "  'Therefore , I have\\na bit spare time in the office .',\n",
       "  'can not it be done earlier than tomorrow ?',\n",
       "  'And\\nso on .',\n",
       "  'And this Japanese businessman was one of the person who was standing in\\nfront of the lines .',\n",
       "  'And people are very kind to me , so I have a very good\\nexperience .',\n",
       "  'Thank you very much .',\n",
       "  'Thank you very much .',\n",
       "  'We were very fortunate\\nescape the night from storm .',\n",
       "  'Gifu stands for Kyoto .',\n",
       "  'What do he do ?',\n",
       "  \"I think it\\n's a math class .\",\n",
       "  \"And I do n't know , but they are really cute , I think .\",\n",
       "  \"So I\\n'm going to find one shouchu for my father .\",\n",
       "  'I think she loves her computer .',\n",
       "  'And there , I copy .',\n",
       "  \"that 's a point , but you know , actually our favorites is\\nalmost same reading books , sing , and part - time job .\",\n",
       "  'whatever happens to he\\nor she , who are from Japan , just smile , do not complain , do not protest ,\\njust smile and say , \" It \\'s OK , OK . \" .',\n",
       "  'And unfortunately , the motorcyclist\\ndropped the cell phone .',\n",
       "  'there are lot of old people .',\n",
       "  'up of this floor ,\\nperhaps there are my two friend do you have another way to get to New York ?',\n",
       "  'how\\nabout your schedule ?',\n",
       "  'I live in Okinawa .',\n",
       "  'Thank you .',\n",
       "  'To help some disabled\\npeople ?',\n",
       "  \"and I 'm not really familiar with them .\",\n",
       "  'And I ran here .',\n",
       "  'I forget they\\nage .',\n",
       "  'And on the way back home in the department store , I found a final sale of\\nshoes .',\n",
       "  \"On the Internet , it was very convenient , but we could n't speak to\\nanyone .\",\n",
       "  'my name is Charlie .',\n",
       "  'I like playing tennis .',\n",
       "  \"I do n't know what to say\\n.\",\n",
       "  'because nowadays , like , antique is only one in the world , most of them .',\n",
       "  'After this test , I get home .',\n",
       "  \"But I could n't stop working .\",\n",
       "  'And unfortunately\\nhis tail lamp was broken from the accident .',\n",
       "  \"I 'm interested in a testing .\",\n",
       "  'In\\nmorning ?',\n",
       "  'thank you .',\n",
       "  'Nice rings .',\n",
       "  \"I tried snowboarding but I did n't do .\",\n",
       "  'just\\nit takes fifteen minutes from my school , yeah .',\n",
       "  'but they have much experiences\\n.',\n",
       "  'in terms of this my say , working room Next time , I really want to join your\\nparty .',\n",
       "  'And the bike guy was left alone .',\n",
       "  'After interview ?',\n",
       "  'So I was doing that\\n.',\n",
       "  'So she choose a good neck tie But I \\'m get used to it already \" , stuff like\\nthat . It \\'s very exciting . no , I take subway to Ueno and then I take J R to\\nthe closest station . I like red one , please . So we decided to stay at the\\nhotel . I have a sore throat my days ? The monkey eat banana . I \\'ve never tried\\nskiing . so please sell me your ticket . Yes , I do . Do you know some action\\nmovie ? Because I \\'m not rich . but I thought I could ask my sisters they help\\nme take care of the cat . Thank you very much . And I do n\\'t know . So I just\\nthought I try Osaka . So I did n\\'t really think of \" OK , I \\'m going to study ,\\n\" but I wanted to see the different culture . So I first watch the movie is ...\\nShinjuku is movie theater in — But they moved to Okinawa . Yes , sometimes . how\\nabout you ? Brazil and Guatemala . no , no , no . fishes , meats , vegetables ,\\nand something like that . maybe I will go to United States by job . So maybe it\\nwill be crowded , I guess . And it blew away almost everything of our camping\\nequipment . My mom fell off with the sickness . So I \\'m really nervous right now\\n. So could you please exchange the ticket or could you give back the money for\\nme ? And when I go to my grandmother house , she feel very happy , so she give\\nme many money . I can not tell you about what you can do after that . It \\'s very\\nfar from here . that was last , maybe . It \\'s OK . we were too sleepy . Because\\nactually , the place I was living was Indianapolis . now , I \\'m learning English\\n. I do n\\'t have special plans . They live slowly and gently in there . but if\\nthere \\'s another chance , it would be nice that you could invite me again . And\\nbefore the war , teachers are very prestigious job . And it \\'s very peaceful and\\nquiet place . I really appreciate what you doed for me , but , frankly speaking\\n, I can not make it today . And from the right , there came a motor cycle .\\nnumber three , we looked menu , and told waitress to my favorite hamburg . So I\\nthink I can find out now . And she always ask me to join them . How often watch\\nthe movie ? But I wanted to buy just tie I did n\\'t have any injure . That was\\ngreat . Local train . the girl using her cell phone . But an hour later , it got\\nworse and everything started to blow and we could n\\'t stay here any more . And\\nmister policeman came to — , and they were explaining what happened there . Then\\nI decide the flight . And she was eating and seeing a monkey . One day last week\\n, that day was raining . In comparison with the transportation in Tokyo , for\\nexample , the time schedule of bus is not correct . I went back to my home town\\nwith my wife . But recently , we do n\\'t do much . I was very hungry , so I\\nimmediately ran to the supermarket to buy some foods . Because I do n\\'t have to\\nwake up very early in the morning . But the waitresses are older lady . when\\nwondering a shop , they may assume that this item is what they think they are ,\\nbut when they bring it home and look through it , sometimes people find out that\\nthe quality of that item is not what they had in mind . I really like it . And\\nit is fifty percent . I think so . Year , I guess so . And the wind got so\\nstrong that our tent was blown off . And looking at the bright side of it , the\\nwhole story , we ended up in a nice hotel room , bright sunshine the next\\nmorning with comfortable environment , enjoying tea . if my sister is fine\\nbecause I have no idea in what kind of state she is in right now . The door is\\nhalf open . The police man understood the situation . Because dog is more\\nhonestly than cat do . And repetition , you know . And , do you know Yurakucho\\nMarion ? Thank you very much . So I do n\\'t really like buy stuff like I —\\nsomething — or no . I like suits . And I was told not to have driver \\'s license\\n. This is the famous Italian restaurant . But it was very good . Then of course\\nthe other person is supposed to be around my car . I \\'m living there with my\\nwife and my one son . So , and I also had to prepare for this test . so I can\\nstand there . You Charlie ? Yeah , it \\'s very hard to walk around at the zoo , I\\nthink . And they \\'re very quick . And my daughter . because not so cold , not so\\nhot . and they said yes , and her persistence has paid off . There are students\\nwho do n\\'t really pay attention because they do n\\'t really care . It \\'s hard\\nnegotiation . And smoking . and ethnically also it \\'s really interesting place .\\nThey just read the textbook and follow the textbook . But , when I need time ,\\nalways holidays come . And when they got there , they built a tent and built a\\nfire and started to cook . And it was .... I will go to the supermarket . And my\\ndog . I \\'d like to take interesting book . this is high school , so I do n\\'t\\nthink many high school students enjoy math , so I do n\\'t know . I have a class\\nright now Economy class , OK . I have only a ten dollars . He was enjoying his\\ndrive . he is sitting and he is working on a personal computer . So it is hard\\nto me to communicate with native people . just to the accident so it can not be\\nhelped . Australia and Beijing and Korea and Moscow and Belgium . One day last\\nweek , woman in her kitchen looked at her refrigerator to see it was empty . And\\n, yes , I think you have scale about for eighty percent to go pass through it .\\nSo on fine days like this ? I just got out of a class in Arabic , and it was\\nkind of difficult it depends . So might be a good day to go shopping , maybe .\\nBut expensive ... , not at all . you can keep the cat . \" .',\n",
       "  \"what 's the point ?\",\n",
       "  'And I stopped the car .',\n",
       "  \"But they did n't .\",\n",
       "  \"one , I think because students are\\nn't interested in mathematics .\",\n",
       "  'And it was like the ticket I bought was five\\nhundred eighty yen .',\n",
       "  'Nice meet you .',\n",
       "  \"Like it 's like a drugstore in Osaka .\",\n",
       "  'Makuhari city .',\n",
       "  'I think that is one of the procedure .',\n",
       "  'By subway .',\n",
       "  'my family is\\nfive , but I have two young sisters .',\n",
       "  \"that 's the solution .\",\n",
       "  'Yes , I will .',\n",
       "  '...',\n",
       "  'describe the picture ?',\n",
       "  \"I could n't help it because it was the train 's problem .\",\n",
       "  \"I do n't like it .\",\n",
       "  'I a bit nervous because a lot of people are here .',\n",
       "  'my major\\nis Shakespeare .',\n",
       "  'In the second row , in the middle , a student is listening to\\nhis , I think , a Walkman .',\n",
       "  \"it 's cooler than in Tokyo .\",\n",
       "  \"And if we do n't know\\neach other , but we are talking each other .\",\n",
       "  \"but I do n't think it 's bad maybe\\n.\",\n",
       "  \"I 'm Ishikawa City , Saitama .\",\n",
       "  'This is just one time'],\n",
       " 101)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences, len(sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
